{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1: Difference between linear regression and logistic regression\n",
        "\n",
        "# Linear regression:\n",
        "# - Used for predicting continuous outcomes.\n",
        "# - Outputs a continuous value.\n",
        "# - Fits a straight line to minimize the mean squared error.\n",
        "\n",
        "# Logistic regression:\n",
        "# - Used for binary or categorical classification problems.\n",
        "# - Outputs probabilities (between 0 and 1) using a sigmoid function.\n",
        "# - Example: Predicting if a customer will buy a product (Yes/No).\n",
        "\n",
        "# Example where logistic regression is more appropriate:\n",
        "# Predicting whether a patient has a disease (1) or not (0) based on medical test results.\n",
        "\n",
        "# Q2: Cost function in logistic regression\n",
        "\n",
        "# - Logistic regression uses a log-loss (cross-entropy) cost function.\n",
        "# - It is optimized using gradient descent or similar methods.\n",
        "# Cost function formula:\n",
        "# J(θ) = -1/m * Σ [y * log(h(x)) + (1 - y) * log(1 - h(x))]\n",
        "\n",
        "# Python code example for sigmoid function:\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Example of cost function implementation:\n",
        "def compute_cost(y, h):\n",
        "    m = len(y)\n",
        "    return -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "\n",
        "# Q3: Regularization in logistic regression\n",
        "\n",
        "# Regularization prevents overfitting by adding a penalty to the cost function.\n",
        "# Two types:\n",
        "# - L1 Regularization (Lasso): Adds absolute value of coefficients to the cost function.\n",
        "# - L2 Regularization (Ridge): Adds squared value of coefficients to the cost function.\n",
        "\n",
        "# Example with L2 Regularization:\n",
        "# J(θ) = -1/m * Σ [y * log(h(x)) + (1 - y) * log(1 - h(x))] + λ/2m * Σ θ^2\n",
        "\n",
        "# Q4: ROC curve\n",
        "\n",
        "# - The ROC (Receiver Operating Characteristic) curve plots True Positive Rate (TPR) vs. False Positive Rate (FPR).\n",
        "# - It evaluates the model's performance at various threshold levels.\n",
        "# - The Area Under the Curve (AUC) indicates the model's ability to distinguish between classes.\n",
        "\n",
        "# Example: Plotting the ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `y_test` and `y_prob` are true labels and predicted probabilities:\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Q5: Feature selection techniques for logistic regression\n",
        "\n",
        "# 1. Recursive Feature Elimination (RFE)\n",
        "# 2. Lasso Regularization (L1 penalty)\n",
        "# 3. Mutual Information or Chi-square test\n",
        "# 4. Removing features with low variance\n",
        "\n",
        "# Example with RFE:\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "selector = RFE(model, n_features_to_select=5)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "selected_features = selector.support_\n",
        "\n",
        "# Q6: Handling imbalanced datasets in logistic regression\n",
        "\n",
        "# Strategies:\n",
        "# 1. Resampling: Oversample minority class or undersample majority class.\n",
        "# 2. Class weighting: Use `class_weight` parameter in scikit-learn's LogisticRegression.\n",
        "# 3. Synthetic Data Generation: Use SMOTE (Synthetic Minority Oversampling Technique).\n",
        "\n",
        "# Example with class weighting:\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Q7: Common issues and challenges in logistic regression\n",
        "\n",
        "# - Multicollinearity: When independent variables are highly correlated.\n",
        "#   Solution: Use Variance Inflation Factor (VIF) to detect multicollinearity and drop one of the correlated features.\n",
        "# - Outliers: Can affect performance. Use robust scaling or remove outliers.\n",
        "# - Non-linearity: Logistic regression assumes linear relationships between features and log-odds.\n",
        "#   Solution: Use feature engineering or switch to non-linear models like decision trees.\n",
        "\n",
        "# Example of VIF calculation:\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import pandas as pd\n",
        "\n",
        "X = pd.DataFrame(X_train)  # Assuming X_train is a numpy array\n",
        "vif = pd.DataFrame()\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "vif[\"Feature\"] = X.columns\n",
        "print(vif)\n"
      ]
    }
  ]
}